{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10834301,"sourceType":"datasetVersion","datasetId":6727998},{"sourceId":10843749,"sourceType":"datasetVersion","datasetId":6734372},{"sourceId":267429,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":228858,"modelId":250611},{"sourceId":267476,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":228898,"modelId":250652},{"sourceId":267821,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":229201,"modelId":250946},{"sourceId":267962,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":229324,"modelId":251067},{"sourceId":268224,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":229539,"modelId":251277},{"sourceId":268312,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":229604,"modelId":251341},{"sourceId":269268,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":230431,"modelId":252174},{"sourceId":269321,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":230475,"modelId":252220},{"sourceId":269423,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":230565,"modelId":252318}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport random\nfrom PIL import Image, ImageOps, ImageFilter\nimport cv2\nimport numpy as np\nfrom skimage.metrics import mean_squared_error, peak_signal_noise_ratio, structural_similarity","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:09:35.018549Z","iopub.execute_input":"2025-02-26T20:09:35.018786Z","iopub.status.idle":"2025-02-26T20:09:40.825153Z","shell.execute_reply.started":"2025-02-26T20:09:35.018765Z","shell.execute_reply":"2025-02-26T20:09:40.824132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Приготовление датасета","metadata":{}},{"cell_type":"code","source":"\"\"\"Функция перевода изображений из тензоров в PIL для инференса\"\"\"\n\ndef tensor_to_pil(tensor_img):\n    unnorm = transforms.Normalize(mean=[-0.5/0.5]*3, std=[1/0.5]*3)\n    tensor_img = unnorm(tensor_img).clamp(0, 1)\n    pil_img = transforms.ToPILImage()(tensor_img)\n    return pil_img\n\n\"\"\"Выгрузка изображений в Dataset\"\"\"\n\nclass PairedImageDataset(Dataset):\n    def __init__(self, dataset_root, image_size=512):\n        self.pairs = []\n        cover_root = os.path.join(dataset_root, \"cover\")\n        final_root = os.path.join(dataset_root, \"final\")\n        for subfolder in sorted(os.listdir(cover_root)):\n            cover_sub = os.path.join(cover_root, subfolder)\n            final_sub = os.path.join(final_root, subfolder)\n            if os.path.isdir(cover_sub) and os.path.isdir(final_sub):\n                cover_files = sorted([f for f in os.listdir(cover_sub) if f.lower().endswith(\".png\")])\n                for fname in cover_files:\n                    cover_path = os.path.join(cover_sub, fname)\n                    final_fname = os.path.splitext(fname)[0] + \".jpg\"\n                    final_path = os.path.join(final_sub, final_fname)\n                    if not os.path.exists(final_path):\n                        final_path = os.path.join(final_sub, fname)\n                    if os.path.exists(final_path):\n                        self.pairs.append((cover_path, final_path))\n                    else:\n                        print(f\"Warning: No matching final image for {cover_path}\")\n        self.transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3),\n        ])\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        cover_path, final_path = self.pairs[idx]\n        cover_img = Image.open(cover_path).convert(\"RGB\")\n        final_img = Image.open(final_path).convert(\"RGB\")\n        return {\n            \"cover\": self.transform(cover_img),\n            \"final\": self.transform(final_img)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:09:40.825884Z","iopub.execute_input":"2025-02-26T20:09:40.826183Z","iopub.status.idle":"2025-02-26T20:09:40.834736Z","shell.execute_reply.started":"2025-02-26T20:09:40.826162Z","shell.execute_reply":"2025-02-26T20:09:40.833743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Функции для инференса","metadata":{}},{"cell_type":"code","source":"# -------------------------\n# Вспомогательные функции\n# -------------------------\n\ndef add_gaussian_noise(image, mean=0, std=2):\n    \"\"\"Добавляет слабый гауссов шум к изображению PIL с обрезкой значений.\"\"\"\n    img_np = np.array(image)  # Преобразуем изображение в массив numpy\n    noise = np.random.normal(mean, std, img_np.shape).astype(np.float32)  # Генерируем шум\n    noisy_img = img_np.astype(np.float32) + noise  # Добавляем шум\n    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)  # Ограничиваем значения в допустимом диапазоне\n    return Image.fromarray(noisy_img)  # Конвертируем обратно в PIL Image\n\ndef apply_perspective_transform(image, distortion_scale=0.1):\n    \"\"\"Применяет случайное перспективное искажение к изображению PIL.\"\"\"\n    img_np = np.array(image)\n    h, w = img_np.shape[:2]  # Получаем размеры изображения\n    \n    # Исходные угловые точки изображения\n    src_points = np.float32([[0, 0], [w, 0], [w, h], [0, h]])\n    \n    # Случайно смещаем углы изображения\n    dst_points = src_points + np.random.uniform(-distortion_scale, distortion_scale, size=(4, 2)) * [w, h]\n    dst_points = dst_points.astype(np.float32)  # Убедимся, что тип данных float32\n    \n    # Вычисляем матрицу перспективного преобразования\n    M = cv2.getPerspectiveTransform(src_points, dst_points)\n    \n    # Применяем перспективное преобразование\n    warped = cv2.warpPerspective(img_np, M, (w, h))\n    \n    return Image.fromarray(warped)  # Возвращаем преобразованное изображение\n\ndef apply_basic_transform(image, max_angle=10, max_pad=0.1, noise_std=2):\n    \"\"\"Применяет базовые аугментации к изображению PIL.\"\"\"\n    # Случайный поворот с темно-серым фоном\n    angle = random.uniform(-max_angle, max_angle)\n    rotated = image.rotate(angle, resample=Image.BICUBIC, expand=True, fillcolor=(10, 10, 10))\n    \n    # Небольшое размытие для сглаживания краев\n    rotated = rotated.filter(ImageFilter.GaussianBlur(radius=0.5))\n    \n    # Добавление случайного паддинга с темно-серым цветом\n    w, h = rotated.size\n    pad_left = int(random.uniform(0, max_pad) * w)\n    pad_right = int(random.uniform(0, max_pad) * w)\n    pad_top = int(random.uniform(0, max_pad) * h)\n    pad_bottom = int(random.uniform(0, max_pad) * h)\n    padded = ImageOps.expand(rotated, border=(pad_left, pad_top, pad_right, pad_bottom), fill=(10, 10, 10))\n    \n    # Добавление небольшого шума\n    if noise_std > 0:\n        padded = add_gaussian_noise(padded, std=noise_std)\n    \n    return padded\n\ndef apply_advanced_transform(image, max_angle=3, max_pad=0.1, distortion_scale=0.1, noise_std=0.2):\n    \"\"\"Применяет сложные аугментации к изображению PIL.\"\"\"\n    # Перспективное искажение\n    persp = apply_perspective_transform(image, distortion_scale)\n    \n    # Случайный поворот с темно-серым фоном\n    angle = random.uniform(-max_angle, max_angle)\n    rotated = persp.rotate(angle, resample=Image.BICUBIC, expand=True, fillcolor=(10, 10, 10))\n    \n    # Размытие для сглаживания краев\n    rotated = rotated.filter(ImageFilter.GaussianBlur(radius=0.5))\n    \n    # Добавление случайного паддинга с темно-серым фоном\n    w, h = rotated.size\n    pad_left = int(random.uniform(0, max_pad) * w)\n    pad_right = int(random.uniform(0, max_pad) * w)\n    pad_top = int(random.uniform(0, max_pad) * h)\n    pad_bottom = int(random.uniform(0, max_pad) * h)\n    padded = ImageOps.expand(rotated, border=(pad_left, pad_top, pad_right, pad_bottom), fill=(10, 10, 10))\n    \n    # Добавление небольшого шума\n    if noise_std > 0:\n        padded = add_gaussian_noise(padded, std=noise_std)\n    \n    return padded\n\ndef compute_metrics(img_target, img_generated):\n    \"\"\"\n    Вычисляет MSE, PSNR и SSIM между двумя изображениями PIL одинакового размера.\n    \"\"\"\n    # Преобразуем изображения в numpy-массивы float32 с нормализацией в диапазоне [0,1]\n    np_target = np.array(img_target).astype(np.float32) / 255.0\n    np_generated = np.array(img_generated).astype(np.float32) / 255.0\n\n    # 1) Среднеквадратичная ошибка (MSE)\n    mse_val = mean_squared_error(np_target, np_generated)\n\n    # 2) Пиковое отношение сигнал/шум (PSNR)\n    psnr_val = peak_signal_noise_ratio(np_target, np_generated, data_range=1.0)\n\n    # 3) Структурное сходство (SSIM)\n    h, w, c = np_target.shape\n    max_win_size = min(h, w, 7)  # Если минимальное измерение < 7, уменьшаем размер окна\n    ssim_val = structural_similarity(\n        np_target, \n        np_generated,\n        data_range=1.0,\n        channel_axis=-1,\n        win_size=max_win_size\n    )\n\n    return mse_val, psnr_val, ssim_val","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Архитектура модели","metadata":{}},{"cell_type":"code","source":"class STN(nn.Module):\n    def __init__(self):\n        super(STN, self).__init__()\n        self.localization = nn.Sequential(\n            nn.Conv2d(3, 8, kernel_size=7),  # Извлечение признаков из входного изображения\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.ReLU(True),\n            nn.AdaptiveAvgPool2d((2, 2))  # Сведение карты активаций к фиксированному размеру\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(10 * 2 * 2, 32),  # Полносвязные слои для вычисления параметров аффинного преобразования\n            nn.ReLU(True),\n            nn.Linear(32, 6)\n        )\n        self.fc[2].weight.data.zero_()  # Инициализация весов последнего слоя нулями\n        self.fc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))  # Инициализация биаса единичной матрицей\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        xs = self.localization(x)\n        xs = xs.view(batch_size, -1)  # Приведение к векторному виду\n        theta = self.fc(xs)\n        theta = theta.view(-1, 2, 3)  # Преобразование в матрицу аффинного преобразования\n        grid = F.affine_grid(theta, x.size(), align_corners=True)\n        x_transformed = F.grid_sample(x, grid, align_corners=True)  # Применение преобразования к входному изображению\n        return x_transformed, theta\n\nclass HDRNet(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, grid_size=16):\n        super(HDRNet, self).__init__()\n        self.grid_size = grid_size  # Размер сетки трансформации (глобальной коррекции)\n        \n        # Локальная ветка (Local branch): предсказывает низкоразрешенную карту преобразования\n        # Использует дилатационные свертки для расширения области восприятия и извлечения дополнительных деталей\n        self.local_branch = nn.Sequential(\n            nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1),  # Базовый сверточный слой\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=2, dilation=2),  # Дилатационная свертка для большего охвата контекста\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Дополнительный слой для уточнения признаков\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, output_nc, kernel_size=3, stride=1, padding=1),  # Предсказание параметров преобразования\n            nn.AdaptiveAvgPool2d((grid_size, grid_size))  # Приведение карты преобразования к низкому разрешению\n        )\n        \n        # Глобальная ветка (Global branch): вычисляет общую коррекцию цвета/тона\n        self.global_branch = nn.Sequential(\n            nn.Conv2d(input_nc, 32, kernel_size=3, stride=2, padding=1),  # Первый сверточный слой с уменьшением размера в 2 раза\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Второй сверточный слой, еще раз уменьшающий размер\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1)  # Приведение к размеру (B, 64, 1, 1) - общий вектор признаков\n        )\n        self.global_fc = nn.Linear(64, output_nc)  # Полносвязный слой для предсказания общей коррекции\n\n    def forward(self, x):\n        # Локальная ветка: предсказывает сетку трансформации и увеличивает ее до размера входного изображения\n        local_grid = self.local_branch(x)  # Выходная форма: (B, output_nc, grid_size, grid_size)\n        local_adjust = nn.functional.interpolate(local_grid, size=x.shape[2:], mode='bilinear', align_corners=True)\n\n        # Глобальная ветка: вычисляет глобальную коррекцию\n        batch_size = x.size(0)\n        global_feat = self.global_branch(x)  # Выходная форма: (B, 64, 1, 1)\n        global_feat = global_feat.view(batch_size, -1)  # Преобразуем вектор в (B, 64)\n        global_adjust = self.global_fc(global_feat)  # Выходная форма: (B, output_nc)\n        global_adjust = global_adjust.unsqueeze(2).unsqueeze(3).expand_as(local_adjust)  # Расширение до размера локальной коррекции\n\n        # Объединяем локальную и глобальную коррекцию\n        adjustment = local_adjust + global_adjust\n        \n        # Остаточное соединение (Residual connection): добавляем рассчитанную коррекцию к входному изображению\n        out = x + adjustment\n        \n        # Применяем активацию tanh для нормализации выходных значений в диапазоне [-1, 1]\n        return torch.tanh(out)\n\n\n\n# Многоуровневый дискриминатор (Multi-Scale Discriminator)\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3):\n        super(NLayerDiscriminator, self).__init__()\n        kw = 4  # Размер ядра свертки\n        padw = 1  # Паддинг\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),  # Первая сверточная операция с уменьшением размера в 2 раза\n            nn.LeakyReLU(0.2, True)  # Активация с утечкой, предотвращает исчезновение градиента\n        ]\n        self.layers = nn.ModuleList([nn.Sequential(*sequence)])\n        nf_mult = 1  # Коэффициент увеличения количества каналов\n\n        # Добавляем сверточные слои с Instance Normalization\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)  # Увеличиваем количество фильтров, но не больше 8 раз\n            self.layers.append(nn.Sequential(\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw),\n                nn.InstanceNorm2d(ndf * nf_mult),  # Нормализация для стабилизации обучения\n                nn.LeakyReLU(0.2, True)\n            ))\n\n        # Дополнительный слой без изменения размера\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        self.layers.append(nn.Sequential(\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw),\n            nn.InstanceNorm2d(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ))\n\n        # Финальный слой, который выдает одно значение для каждой области изображения\n        self.output_layer = nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)\n        \n    def forward(self, input):\n        result = []\n        x = input\n        for layer in self.layers:\n            x = layer(x)\n            result.append(x)  # Сохраняем промежуточные карты активаций\n        out = self.output_layer(x)\n        result.append(out)  # Добавляем финальный выход\n        return result  # Возвращаем список промежуточных карт признаков и финальный выход\n\n\n# Многоуровневый дискриминатор, использующий несколько NLayerDiscriminator на разных масштабах изображения\nclass MultiScaleDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, num_D=3):\n        super(MultiScaleDiscriminator, self).__init__()\n        self.num_D = num_D  # Количество дискриминаторов\n        self.discriminators = nn.ModuleList()\n\n        # Создаем num_D экземпляров NLayerDiscriminator\n        for _ in range(num_D):\n            self.discriminators.append(NLayerDiscriminator(input_nc, ndf, n_layers))\n\n        # Среднее сглаживание (downsampling) для понижения разрешения изображения\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n        \n    def forward(self, input):\n        results = []\n        for D in self.discriminators:\n            results.append(D(input))  # Обрабатываем вход через дискриминатор\n            input = self.downsample(input)  # Уменьшаем размер изображения перед подачей в следующий дискриминатор\n        return results  # Возвращаем выходы всех дискриминаторов","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:09:40.836391Z","iopub.execute_input":"2025-02-26T20:09:40.836695Z","iopub.status.idle":"2025-02-26T20:09:40.857128Z","shell.execute_reply.started":"2025-02-26T20:09:40.836644Z","shell.execute_reply":"2025-02-26T20:09:40.856343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Обучение модели","metadata":{}},{"cell_type":"markdown","source":"Параметры","metadata":{}},{"cell_type":"code","source":"num_epochs = 200\nbatch_size = 4\nlr = 1e-4\nlr_G = lr\nlambda_l1 = 2.0      # Вес L1-ошибки (реконструкция)\nlambda_feat = 10.0   # Вес Feature Matching Loss\n\ndataset_root = \"/kaggle/input/screen2photo-dataset-augm-and-sep/augment_train_data/kaggle/working/augment_train_data\"\nkaggle_output_dir = \"/kaggle/working\"\n\ncheckpoint_interval = 10  # Интервал сохранения чекпоинтов\n#Путь до модели в формате .pth. Если обучение с нуля, то выставьте resume_checkpoint = None\nresume_checkpoint = '/kaggle/input/pseudo_hdrnetstn-ver-final-ep-20/pytorch/default/1/checkpoint_hdrnet_v2_epoch_20.pth'\nstart_epoch = 20 #с какой эпохи стартует чекпоинт (если с нуля, то выставьте 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_size_for_model = 256  #не менять\ntrain_dataset = PairedImageDataset(dataset_root, image_size=img_size_for_model)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\n# -------------------------\n# Функции загрузки/сохранения чекпоинтов\n# -------------------------\ndef load_checkpoint(checkpoint_path, generator, discriminator, stn_module, optimizer_G, optimizer_D):\n    print(f\"Загружаем чекпоинт {checkpoint_path} ...\")\n    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n    \n    generator.load_state_dict(checkpoint[\"generator\"])\n    discriminator.load_state_dict(checkpoint[\"discriminator\"])\n    optimizer_G.load_state_dict(checkpoint[\"optimizer_G\"])\n    optimizer_D.load_state_dict(checkpoint[\"optimizer_D\"])\n    \n    start_epoch = checkpoint[\"epoch\"]\n    print(f\"Чекпоинт загружен. Продолжаем обучение с {start_epoch} эпохи.\\n\")\n    return start_epoch\n\ndef save_checkpoint(checkpoint_path, epoch, generator, discriminator, stn_module, optimizer_G, optimizer_D):\n    print(f\"Сохраняем чекпоинт в {checkpoint_path} ...\")\n    checkpoint = {\n        \"epoch\": epoch,\n        \"generator\": generator.state_dict(),\n        \"discriminator\": discriminator.state_dict(),\n        \"stn_module\": stn_module.state_dict(),\n        \"optimizer_G\": optimizer_G.state_dict(),\n        \"optimizer_D\": optimizer_D.state_dict()\n    }\n    torch.save(checkpoint, checkpoint_path)\n    print(\"Чекпоинт сохранен.\\n\")\n\n# -------------------------\n# Настройки устройства\n# -------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Используется устройство:\", device)\n\n# Инициализация моделей\ngenerator = HDRNet(input_nc=3, output_nc=3).to(device)\nstn_module = STN().to(device)\n\n# Многоуровневый дискриминатор (Multi-Scale Discriminator)\ndiscriminator = MultiScaleDiscriminator(input_nc=6, ndf=64, n_layers=3, num_D=3).to(device)\n\n# Функции потерь\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_L1 = nn.L1Loss()\n\n# Оптимизаторы (учим и HDRNet, и STN)\noptimizer_G = optim.Adam(list(generator.parameters()) + list(stn_module.parameters()), lr=lr_G, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n# -------------------------\n# Загрузка чекпоинта, если продолжаем обучение\n# -------------------------\nif resume_checkpoint is not None and os.path.exists(resume_checkpoint):\n    start_epoch = load_checkpoint(resume_checkpoint, generator, discriminator, stn_module, optimizer_G, optimizer_D)\n\n# Замораживание STN после определенной эпохи\nfreeze_stn_epoch = 200\nif start_epoch >= freeze_stn_epoch:\n    for param in stn_module.parameters():\n        param.requires_grad = False\n    print(f\"STN заморожен (обучение возобновлено после {freeze_stn_epoch} эпохи)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:09:40.858139Z","iopub.execute_input":"2025-02-26T20:09:40.858413Z","iopub.status.idle":"2025-02-26T20:09:46.700094Z","shell.execute_reply.started":"2025-02-26T20:09:40.858385Z","shell.execute_reply":"2025-02-26T20:09:46.699201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(start_epoch, num_epochs):\n    generator.train()\n    stn_module.train()\n    discriminator.train()\n    \n    # Замораживание STN после определенной эпохи\n    if epoch >= freeze_stn_epoch:\n        for param in stn_module.parameters():\n            param.requires_grad = False\n\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    \n    for batch in progress_bar:\n        real_A = batch[\"cover\"].to(device)   # Входное изображение (экран)\n        real_B = batch[\"final\"].to(device)   # Целевое изображение (фото экрана)\n\n        # STN исправляет перспективные искажения в real_B\n        fixed_real_B, theta_real = stn_module(real_B)\n\n        # Генерация изображения из real_A с помощью HDRNet\n        fake_B = generator(real_A)\n\n        # Подготовка данных для дискриминатора (объединяем вход и целевое изображение)\n        real_AB = torch.cat([real_A, fixed_real_B], dim=1)  # Реальная пара\n        fake_AB = torch.cat([real_A, fake_B], dim=1)        # Фейковая пара\n\n        # -----------------\n        # Обучение генератора\n        # -----------------\n        optimizer_G.zero_grad()\n\n        # GAN-ошибка (насколько хорошо фейковое изображение обманывает дискриминатор)\n        pred_fake = discriminator(fake_AB)\n        with torch.no_grad():\n            pred_real = discriminator(real_AB)\n\n        loss_GAN = 0.0\n        for scale_out in pred_fake:\n            out = scale_out[-1]\n            valid_ = torch.ones_like(out, device=device)\n            loss_GAN += criterion_GAN(out, valid_)\n        loss_GAN /= discriminator.num_D\n\n        # Ошибка сопоставления признаков (Feature Matching Loss)\n        loss_FM = 0.0\n        for i in range(discriminator.num_D):\n            num_intermediate = len(pred_fake[i]) - 1\n            for j in range(num_intermediate):\n                loss_FM += criterion_L1(pred_fake[i][j], pred_real[i][j])\n        loss_FM /= (discriminator.num_D * num_intermediate)\n\n        # L1-ошибка между сгенерированным изображением и исправленным реальным изображением\n        loss_L1_ = criterion_L1(fake_B, fixed_real_B)\n\n        # Итоговая функция ошибки генератора\n        loss_G = loss_GAN + lambda_l1 * loss_L1_ + lambda_feat * loss_FM\n        loss_G.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        # Обучение дискриминатора\n        # ---------------------\n        optimizer_D.zero_grad()\n        pred_fake = discriminator(fake_AB.detach())  # Отключаем градиенты для генератора\n\n        loss_D = 0.0\n        for i in range(discriminator.num_D):\n            out_real = pred_real[i][-1]\n            out_fake = pred_fake[i][-1]\n            valid_ = torch.ones_like(out_real, device=device)\n            fake_ = torch.zeros_like(out_fake, device=device)\n            loss_real = criterion_GAN(out_real, valid_)\n            loss_fake = criterion_GAN(out_fake, fake_)\n            loss_D += 0.5 * (loss_real + loss_fake)\n        loss_D /= discriminator.num_D\n        loss_D.backward()\n        optimizer_D.step()\n\n        # Отображение текущих ошибок\n        progress_bar.set_postfix({\n            \"loss_G\": loss_G.item(),\n            \"loss_D\": loss_D.item(),\n            \"L1\": loss_L1_.item(),\n            \"FM\": loss_FM.item()\n        })\n\n    # Вывод промежуточных результатов каждые 5 эпох\n    if (epoch + 1) % 5 == 0:\n        print(f\"[INFO] Epoch {epoch+1}/{num_epochs} completed. Loss_G: {loss_G.item():.4f}, Loss_D: {loss_D.item():.4f}\")\n\n    # Сохранение чекпоинта каждые checkpoint_interval эпох\n    if (epoch + 1) % checkpoint_interval == 0:\n        ckpt_path = os.path.join(kaggle_output_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n        save_checkpoint(ckpt_path, epoch+1, generator, discriminator, stn_module, optimizer_G, optimizer_D)\n\n# Финальное сохранение модели\nfinal_ckpt_path = os.path.join(kaggle_output_dir, f\"checkpoint_epoch_{num_epochs}.pth\")\nsave_checkpoint(final_ckpt_path, num_epochs, generator, discriminator, stn_module, optimizer_G, optimizer_D)\nprint(f\"Обучение завершено. Финальный чекпоинт сохранен в {final_ckpt_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T19:13:38.889868Z","iopub.execute_input":"2025-02-26T19:13:38.890095Z","execution_failed":"2025-02-26T19:36:28.787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Инференс","metadata":{}},{"cell_type":"code","source":"advanced_mod = True\n# Путь к датасету для инференса\ninference_dataset_root = \"/kaggle/input/screen2photo-dataset-augm-and-sep/inference_data/kaggle/working/inference_data\"\n\nuse_uploaded_model = True  \n# Путь к чекпоинту для инференса\nuploaded_checkpoint_path = \"/kaggle/input/111/pytorch/default/1/checkpoint_epoch_30 (1).pth\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_full_dataset = PairedImageDataset(inference_dataset_root, image_size=img_size_for_model)\nprint(f\"Загружен инференс-датасет с {len(inference_full_dataset)} парами изображений.\")\n\n# Выбор функции трансформации (простая или продвинутая)\nif advanced_mod:\n    transform_func = apply_advanced_transform\nelse:\n    transform_func = apply_basic_transform  # Можно переключить на advanced, если нужно\n\n\n# -------------------------\n# Загрузка предобученной модели\n# -------------------------\n\nif use_uploaded_model:\n    ckpt = torch.load(uploaded_checkpoint_path, map_location=device, weights_only=True)\n    generator.load_state_dict(ckpt[\"generator\"])  # Загружаем только веса генератора\n    print(f\"Загружены веса генератора из: {uploaded_checkpoint_path}\")\n\ngenerator.eval()  # Переводим генератор в режим инференса\n\n# -------------------------\n# Инференс и вычисление метрик\n# -------------------------\n\nmse_list, psnr_list, ssim_list = [], [], []\n\nfor idx in range(len(inference_full_dataset)):\n    sample = inference_full_dataset[idx]\n    cover_tensor = sample[\"cover\"].to(device)  # Входное изображение (экран)\n    final_tensor = sample[\"final\"].to(device)  # Целевое изображение (фото экрана)\n\n    with torch.no_grad():\n        fake_tensor = generator(cover_tensor.unsqueeze(0)).squeeze(0)  # Генерируем изображение\n\n    # Преобразуем тензор в изображение PIL\n    fake_image = tensor_to_pil(fake_tensor.cpu())\n\n    # Применяем выбранную трансформацию\n    transformed_image = transform_func(\n        fake_image,\n        max_angle=3,\n        max_pad=0.01,\n        distortion_scale=0.05,\n        noise_std=0.1\n    )\n\n    # Преобразуем целевое изображение в PIL\n    final_image = tensor_to_pil(final_tensor.cpu())\n\n    # Приводим целевое изображение к размеру трансформированного\n    w, h = transformed_image.size\n    final_resized = final_image.resize((w, h), Image.Resampling.BILINEAR)\n\n    # Вычисляем метрики\n    mse_val, psnr_val, ssim_val = compute_metrics(final_resized, transformed_image)\n    mse_list.append(mse_val)\n    psnr_list.append(psnr_val)\n    ssim_list.append(ssim_val)\n\n    # Визуализация изображений (по желанию)\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    axes[0].imshow(tensor_to_pil(cover_tensor.cpu()))\n    axes[0].set_title(\"Cover (Input)\")\n    axes[0].axis(\"off\")\n\n    axes[1].imshow(transformed_image)\n    axes[1].set_title(\"Transformed (Generated)\")\n    axes[1].axis(\"off\")\n\n    axes[2].imshow(final_image)\n    axes[2].set_title(\"Final (Target)\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n\n# -------------------------\n# Вывод средних метрик по всему датасету\n# -------------------------\n\nN = len(mse_list)\navg_mse = sum(mse_list) / N\navg_psnr = sum(psnr_list) / N\navg_ssim = sum(ssim_list) / N\n\nprint(f\"Среднее MSE : {avg_mse:.4f}\")\nprint(f\"Среднее PSNR: {avg_psnr:.2f} dB\")\nprint(f\"Среднее SSIM: {avg_ssim:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:17:13.384440Z","iopub.execute_input":"2025-02-26T20:17:13.384806Z","iopub.status.idle":"2025-02-26T20:17:40.593281Z","shell.execute_reply.started":"2025-02-26T20:17:13.384780Z","shell.execute_reply":"2025-02-26T20:17:40.592353Z"}},"outputs":[],"execution_count":null}]}